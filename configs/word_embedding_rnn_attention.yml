# global parameters
global_parameters:
  # model related
  - &model_arch_type 'RnnAttentionModel'  # 可选的模型['FastText'，'TextCNN'，'']
  - &train true #[true,false] # 如果不使用transformers model，该参数表示是否训练词向量，如果使用transformers model，该参数表示是否对transformers model 进行微调
  - &dropout 0.5
  - &class_num 2
  # data related
  - &dataset_type 'EmbeddingDataset'
  - &data_dir './htqe_data'
  - &cache_dir './transformer_models/.cache'
  - &overwrite_cache false
  - &word_embedding './word_embedding/.cache/htqe_word_embedding.pkl' # [weibo_word_embedding.pkl, weibo_char_embedding.pkl]
  - &num_workers 16
  - &batch_size 5


experiment_name: *model_arch_type
num_gpu: 1                         # GPU数量
device_id: '0'
visual_device: '0'
main_device_id: '0'
resume_path: null                         # path to latest checkpoint

# 模型
model_arch:
  type: *model_arch_type
  args:
    word_embedding: *word_embedding
    train: *train
    dropout: *dropout
    # rnn attention parameters
    rnn_type: 'lstm'  # [rnn,lstm,gru]
    hidden_dim: 256
    n_layers: 4
    bidirectional: true
    batch_first: true
    class_num: *class_num

train_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'htqe_train.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    word_embedding: *word_embedding
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker

valid_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'htqe_test.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    word_embedding: *word_embedding
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker

test_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'htqe_test.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    word_embedding: *word_embedding
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker


optimizer:
  type: 'AdamW'
  args:
    lr: 0.001

lr_scheduler:
  type: 'get_linear_schedule_with_warmup'
  args:
    num_warmup_steps: 0

loss:
  - "ce_loss"

metrics:
  - "categorical_accuracy"

trainer:
  epochs: 3
  save_dir: 'saved/'
  save_period: 1
  verbosity: 2
  monitor: "max val_categorical_accuracy"
  early_stop: 5
  tensorboard: true

