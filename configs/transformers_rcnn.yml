# global parameters
global_parameters:
  # model related
  - &model_arch_type 'TransformersRCNN'
  - &is_train true #[true,false] # 如果不使用transformers model，该参数表示是否训练词向量，如果使用transformers model，该参数表示是否对transformers model 进行微调
  - &class_num 2
  # data related
  - &dataset_type 'TransformersDataset'
  - &data_dir './htqe_data/' # 定义锚
  - &cache_dir './htqe_data/.cache'
  - &overwrite_cache false   # 是否覆盖已经处理好的数据缓存。当数据处理耗费时间较长时，通过配置该参数为false，可再下次加载数据时，直接加载处理好的数据缓存。从而节省不必要的时间浪费
  - &transformer_model 'bert-base-multilingual-uncased' # 详情参见： https://huggingface.co/models
  - &force_download false
  - &num_workers 32
  - &batch_size 64


experiment_name: *model_arch_type
num_gpu: 1                         # GPU数量
device_id: '0'
visual_device: '0'
main_device_id: '0'
resume_path: null                         # path to latest checkpoint

# 模型
model_arch:
  type: *model_arch_type
  args:
    transformer_model: *transformer_model
    cache_dir: *cache_dir
    force_download: *force_download
    is_train: *is_train
    class_num: *class_num
    # rnn configure
    rnn_type: 'lstm'  # [rnn,lstm,gru]
    hidden_dim: 256
    n_layers: 2
    bidirectional: true
    batch_first: true
    dropout: 0.5

train_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'htqe_train.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    transformer_model: *transformer_model
    force_download: *force_download
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker

valid_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'htqe_valid.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    transformer_model: *transformer_model
    force_download: *force_download
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker

test_set:
  type: *dataset_type
  args:
    data_dir: *data_dir
    file_name: 'htqe_test.jsonl'
    cache_dir: *cache_dir
    overwrite_cache: *overwrite_cache
    transformer_model: *transformer_model
    force_download: *force_download
    shuffle: true
    batch_size: *batch_size   # data loader batch size
    num_workers: *num_workers # data loader num of worker


optimizer:
  type: 'AdamW'
  args:
    lr: 0.001

lr_scheduler:
  type: 'get_linear_schedule_with_warmup'
  args:
    num_warmup_steps: 0

loss:
  - "ce_loss"

metrics:
  - "categorical_accuracy"

trainer:
  epochs: 3
  save_dir: 'saved/'
  save_period: 1
  verbosity: 2
  monitor: "max val_categorical_accuracy"
  early_stop: 20
  tensorboard: true

